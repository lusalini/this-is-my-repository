{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics, cross-validation and parameter optimization.\n",
    "\n",
    "## 0 So, who has implemented accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Simple evaluation for a regression problem\n",
    "\n",
    "For a _regression_ problem, accuracy is not a meaningful measure of performance.\n",
    "\n",
    "**Question for you: Why?**\n",
    "\n",
    "Load a regression dataset, this is all review from last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(102, 13)\n",
      "(404,)\n",
      "(102,)\n"
     ]
    }
   ],
   "source": [
    "# load a regression dataset, same as last time\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# check shapes\n",
    "for v in (X_train, X_test, y_train, y_test):\n",
    "    print v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# check parameters of the estimator\n",
    "print clf\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean absolute error\n",
    "\n",
    "A popular and simple way to evaluate regression models are variants of **mean error**. For each pair of true y value of predicted value, \"error\" simply means the difference between the true and predicted value. Of all those error values, you can simply take the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.74913846814\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "print mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that this functions works the way we described it above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7491384681430215"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "error = numpy.abs(y_test - y_pred)\n",
    "error.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the lowest possible error?\n",
    "mean_absolute_error(y_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you: what is the highest possible mean absolute error?**\n",
    "\n",
    "### Mean squared error\n",
    "\n",
    "Mean absolute error gives equal weight for errors of any size. But intuitively, errors can be small or large, and large deviations from the true values are worse than almost getting the correct result. **Mean squared error** puts this idea into practice. Incidentally, squaring the differences also gets rid of negative error values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.3745630297\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root mean squared error (RMSE)\n",
    "\n",
    "Finally, you can take the root of the mean squared error (RMSE), as follows. RMSE is an important measure of regression error, and relatively easy, as long as you remember to apply everything backwards (compute error, square the differences, then take the mean, then take the square root)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.83472471085\n"
     ]
    }
   ],
   "source": [
    "print numpy.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the square root at the very end is similar to taking the square root to obtain standard deviation from variance.\n",
    "\n",
    "## 2 Cross-validation\n",
    "\n",
    "Until now, we have assigned observations to the training or test set at random. This creates some unwanted variance, as our model could be trained on easy examples and tested on hard ones, or vice versa - purely by chance. Cross-validation is a method to overcome this difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load classification dataset, all review\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notion of data folds\n",
    "\n",
    "The procedure builds on a class called `KFold`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object split at 0x10d0477d0>\n",
      "[  0   1   2   3   5   6   8  10  11  12  13  14  15  16  17  19  22  24\n",
      "  25  27  28  29  30  31  32  33  35  36  37  39  40  41  42  43  44  45\n",
      "  46  48  50  51  52  53  54  55  57  58  59  60  61  62  63  64  65  66\n",
      "  69  70  71  72  73  74  75  77  78  81  82  83  84  85  86  87  88  89\n",
      "  91  93  94  95  96  99 100 101 102 103 104 105 107 108 109 110 111 113\n",
      " 114 115 116 117 118 119 120 121 122 124 127 128 129 130 131 133 135 136\n",
      " 137 138 140 141 142 143 144 145 146 147 148 149] [  4   7   9  18  20  21  23  26  34  38  47  49  56  67  68  76  79  80\n",
      "  90  92  97  98 106 112 123 125 126 132 134 139]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
      "  20  21  22  23  24  25  26  28  30  31  32  33  34  36  38  39  40  41\n",
      "  42  43  44  45  46  47  49  50  52  53  54  56  57  58  60  61  62  64\n",
      "  66  67  68  70  72  73  74  75  76  77  78  79  80  81  84  86  87  88\n",
      "  89  90  92  94  95  96  97  98 100 101 102 103 104 105 106 108 109 111\n",
      " 112 113 114 116 117 118 119 120 121 123 124 125 126 127 129 130 131 132\n",
      " 133 134 135 136 137 139 140 142 144 146 147 149] [ 13  19  27  29  35  37  48  51  55  59  63  65  69  71  82  83  85  91\n",
      "  93  99 107 110 115 122 128 138 141 143 145 148]\n",
      "[  0   2   3   4   5   6   7   8   9  11  13  15  16  18  19  20  21  23\n",
      "  25  26  27  28  29  30  34  35  36  37  38  39  40  41  42  43  44  45\n",
      "  46  47  48  49  50  51  52  55  56  58  59  60  61  62  63  64  65  66\n",
      "  67  68  69  70  71  72  73  75  76  77  78  79  80  81  82  83  85  86\n",
      "  87  89  90  91  92  93  95  97  98  99 101 103 104 105 106 107 108 109\n",
      " 110 112 113 114 115 116 117 118 121 122 123 124 125 126 127 128 130 131\n",
      " 132 133 134 136 138 139 140 141 143 144 145 148] [  1  10  12  14  17  22  24  31  32  33  53  54  57  74  84  88  94  96\n",
      " 100 102 111 119 120 129 135 137 142 146 147 149]\n",
      "[  1   3   4   5   7   9  10  11  12  13  14  17  18  19  20  21  22  23\n",
      "  24  25  26  27  29  30  31  32  33  34  35  37  38  43  45  47  48  49\n",
      "  51  52  53  54  55  56  57  59  60  61  62  63  65  66  67  68  69  71\n",
      "  74  75  76  77  78  79  80  81  82  83  84  85  88  90  91  92  93  94\n",
      "  95  96  97  98  99 100 102 105 106 107 108 109 110 111 112 113 115 116\n",
      " 117 118 119 120 122 123 125 126 127 128 129 130 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149] [  0   2   6   8  15  16  28  36  39  40  41  42  44  46  50  58  64  70\n",
      "  72  73  86  87  89 101 103 104 114 121 124 131]\n",
      "[  0   1   2   4   6   7   8   9  10  12  13  14  15  16  17  18  19  20\n",
      "  21  22  23  24  26  27  28  29  31  32  33  34  35  36  37  38  39  40\n",
      "  41  42  44  46  47  48  49  50  51  53  54  55  56  57  58  59  63  64\n",
      "  65  67  68  69  70  71  72  73  74  76  79  80  82  83  84  85  86  87\n",
      "  88  89  90  91  92  93  94  96  97  98  99 100 101 102 103 104 106 107\n",
      " 110 111 112 114 115 119 120 121 122 123 124 125 126 128 129 131 132 134\n",
      " 135 137 138 139 141 142 143 145 146 147 148 149] [  3   5  11  25  30  43  45  52  60  61  62  66  75  77  78  81  95 105\n",
      " 108 109 113 116 117 118 127 130 133 136 140 144]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "indexes = kf.split(X, y)\n",
    "print indexes\n",
    "for train_indexes, test_indexes in indexes:\n",
    "    print train_indexes, test_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `split` method of a `KFold` object returns lists of indexes that can be used to index the `X` and `y` arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   4   6   7   8   9  10  12  13  14  15  16  17  18  19  20\n",
      "  21  22  23  24  26  27  28  29  31  32  33  34  35  36  37  38  39  40\n",
      "  41  42  44  46  47  48  49  50  51  53  54  55  56  57  58  59  63  64\n",
      "  65  67  68  69  70  71  72  73  74  76  79  80  82  83  84  85  86  87\n",
      "  88  89  90  91  92  93  94  96  97  98  99 100 101 102 103 104 106 107\n",
      " 110 111 112 114 115 119 120 121 122 123 124 125 126 128 129 131 132 134\n",
      " 135 137 138 139 141 142 143 145 146 147 148 149]\n",
      "\n",
      "[[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 5.   3.6  1.4  0.2]\n",
      " [ 4.6  3.4  1.4  0.3]\n",
      " [ 5.   3.4  1.5  0.2]\n",
      " [ 4.4  2.9  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 5.4  3.7  1.5  0.2]\n",
      " [ 4.8  3.   1.4  0.1]\n",
      " [ 4.3  3.   1.1  0.1]\n",
      " [ 5.8  4.   1.2  0.2]\n",
      " [ 5.7  4.4  1.5  0.4]\n",
      " [ 5.4  3.9  1.3  0.4]\n",
      " [ 5.1  3.5  1.4  0.3]\n",
      " [ 5.7  3.8  1.7  0.3]\n",
      " [ 5.1  3.8  1.5  0.3]\n",
      " [ 5.4  3.4  1.7  0.2]\n",
      " [ 5.1  3.7  1.5  0.4]\n",
      " [ 4.6  3.6  1.   0.2]\n",
      " [ 5.1  3.3  1.7  0.5]\n",
      " [ 4.8  3.4  1.9  0.2]\n",
      " [ 5.   3.4  1.6  0.4]\n",
      " [ 5.2  3.5  1.5  0.2]\n",
      " [ 5.2  3.4  1.4  0.2]\n",
      " [ 4.7  3.2  1.6  0.2]\n",
      " [ 5.4  3.4  1.5  0.4]\n",
      " [ 5.2  4.1  1.5  0.1]\n",
      " [ 5.5  4.2  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 5.   3.2  1.2  0.2]\n",
      " [ 5.5  3.5  1.3  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 4.4  3.   1.3  0.2]\n",
      " [ 5.1  3.4  1.5  0.2]\n",
      " [ 5.   3.5  1.3  0.3]\n",
      " [ 4.5  2.3  1.3  0.3]\n",
      " [ 4.4  3.2  1.3  0.2]\n",
      " [ 5.1  3.8  1.9  0.4]\n",
      " [ 5.1  3.8  1.6  0.2]\n",
      " [ 4.6  3.2  1.4  0.2]\n",
      " [ 5.3  3.7  1.5  0.2]\n",
      " [ 5.   3.3  1.4  0.2]\n",
      " [ 7.   3.2  4.7  1.4]\n",
      " [ 6.4  3.2  4.5  1.5]\n",
      " [ 5.5  2.3  4.   1.3]\n",
      " [ 6.5  2.8  4.6  1.5]\n",
      " [ 5.7  2.8  4.5  1.3]\n",
      " [ 6.3  3.3  4.7  1.6]\n",
      " [ 4.9  2.4  3.3  1. ]\n",
      " [ 6.6  2.9  4.6  1.3]\n",
      " [ 5.2  2.7  3.9  1.4]\n",
      " [ 6.1  2.9  4.7  1.4]\n",
      " [ 5.6  2.9  3.6  1.3]\n",
      " [ 6.7  3.1  4.4  1.4]\n",
      " [ 5.8  2.7  4.1  1. ]\n",
      " [ 6.2  2.2  4.5  1.5]\n",
      " [ 5.6  2.5  3.9  1.1]\n",
      " [ 5.9  3.2  4.8  1.8]\n",
      " [ 6.1  2.8  4.   1.3]\n",
      " [ 6.3  2.5  4.9  1.5]\n",
      " [ 6.1  2.8  4.7  1.2]\n",
      " [ 6.4  2.9  4.3  1.3]\n",
      " [ 6.8  2.8  4.8  1.4]\n",
      " [ 5.7  2.6  3.5  1. ]\n",
      " [ 5.5  2.4  3.8  1.1]\n",
      " [ 5.8  2.7  3.9  1.2]\n",
      " [ 6.   2.7  5.1  1.6]\n",
      " [ 5.4  3.   4.5  1.5]\n",
      " [ 6.   3.4  4.5  1.6]\n",
      " [ 6.7  3.1  4.7  1.5]\n",
      " [ 6.3  2.3  4.4  1.3]\n",
      " [ 5.6  3.   4.1  1.3]\n",
      " [ 5.5  2.5  4.   1.3]\n",
      " [ 5.5  2.6  4.4  1.2]\n",
      " [ 6.1  3.   4.6  1.4]\n",
      " [ 5.8  2.6  4.   1.2]\n",
      " [ 5.   2.3  3.3  1. ]\n",
      " [ 5.6  2.7  4.2  1.3]\n",
      " [ 5.7  2.9  4.2  1.3]\n",
      " [ 6.2  2.9  4.3  1.3]\n",
      " [ 5.1  2.5  3.   1.1]\n",
      " [ 5.7  2.8  4.1  1.3]\n",
      " [ 6.3  3.3  6.   2.5]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 7.1  3.   5.9  2.1]\n",
      " [ 6.3  2.9  5.6  1.8]\n",
      " [ 6.5  3.   5.8  2.2]\n",
      " [ 4.9  2.5  4.5  1.7]\n",
      " [ 7.3  2.9  6.3  1.8]\n",
      " [ 6.5  3.2  5.1  2. ]\n",
      " [ 6.4  2.7  5.3  1.9]\n",
      " [ 6.8  3.   5.5  2.1]\n",
      " [ 5.8  2.8  5.1  2.4]\n",
      " [ 6.4  3.2  5.3  2.3]\n",
      " [ 6.   2.2  5.   1.5]\n",
      " [ 6.9  3.2  5.7  2.3]\n",
      " [ 5.6  2.8  4.9  2. ]\n",
      " [ 7.7  2.8  6.7  2. ]\n",
      " [ 6.3  2.7  4.9  1.8]\n",
      " [ 6.7  3.3  5.7  2.1]\n",
      " [ 7.2  3.2  6.   1.8]\n",
      " [ 6.2  2.8  4.8  1.8]\n",
      " [ 6.4  2.8  5.6  2.1]\n",
      " [ 7.2  3.   5.8  1.6]\n",
      " [ 7.9  3.8  6.4  2. ]\n",
      " [ 6.4  2.8  5.6  2.2]\n",
      " [ 6.1  2.6  5.6  1.4]\n",
      " [ 7.7  3.   6.1  2.3]\n",
      " [ 6.4  3.1  5.5  1.8]\n",
      " [ 6.   3.   4.8  1.8]\n",
      " [ 6.9  3.1  5.4  2.1]\n",
      " [ 6.9  3.1  5.1  2.3]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 6.8  3.2  5.9  2.3]\n",
      " [ 6.7  3.   5.2  2.3]\n",
      " [ 6.3  2.5  5.   1.9]\n",
      " [ 6.5  3.   5.2  2. ]\n",
      " [ 6.2  3.4  5.4  2.3]\n",
      " [ 5.9  3.   5.1  1.8]]\n"
     ]
    }
   ],
   "source": [
    "# exploiting the fact that loop variables leak into the outer environment\n",
    "print train_indexes\n",
    "print\n",
    "print X[train_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic folds and cross-validation\n",
    "\n",
    "The process of assigning observations to folds, training models and testing them on the respective portion that was held out (the examples at the test set indexes) can be automated of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(50,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# First decide on an estimator\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50, )) # `hidden_layer_sizes` is a hyperparameter\n",
    "print mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          1.          1.          0.86666667  0.8         1.\n",
      "  0.86666667  1.          1.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation with `cross_val_score`\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(mlp, X, y, cv=10, scoring='accuracy')\n",
    "print scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives an accuracy score for each time data was split into training and testing examples. Averaging them will give a better (= more smooth) estimate of generalization, or \"out-of-sample performance\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95333333333333337"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you: what is a reasonable number of folds k, for k-fold cross validation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Automated search for hyperparameters\n",
    "\n",
    "Until now, when we initialized estimators (like KNN, or MLP) we simply brushed over the fact that certain parameters need to be set. For instance, KNN needs the number of neighbors `n_neighbors`, and an MLP very typically needs to know the dimensions of hidden layers, `hidden_layer_sizes`.\n",
    "\n",
    "Using the toolset we have aquired until now, we can loop over different values of those so-called **hyperparameters** and get a mean cross-validation score in each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
      "[0.81999999999999995, 0.93333333333333335, 0.92666666666666675, 0.96666666666666656, 0.95999999999999996, 0.97333333333333338, 0.98666666666666669, 0.96666666666666679, 0.97333333333333338]\n"
     ]
    }
   ],
   "source": [
    "# search for an optimal value of hidden layer size for MLP:\n",
    "layer_size_range = range(20, 110, 10)\n",
    "print layer_size_range\n",
    "\n",
    "mean_scores = []\n",
    "\n",
    "for s in layer_size_range:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(s))\n",
    "    score = cross_val_score(mlp, X, y, cv=10, scoring='accuracy')\n",
    "    mean_scores.append(score.mean())\n",
    "\n",
    "print mean_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, setting the hyperparameter `hidden_layer_sizes` to a reasonable value is crucial to get a good accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,u'Cross-validated accuracy')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAF3CAYAAACPC83LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a19c97a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.plot(layer_size_range, mean_scores)\n",
    "\n",
    "def annot_max(x,y, ax=None):\n",
    "    \"\"\"\n",
    "    This code is adapted from:\n",
    "    https://stackoverflow.com/a/43375318/1987598\n",
    "    \"\"\"\n",
    "    xmax = x[numpy.argmax(y)]\n",
    "    ymax = max(y)\n",
    "    text= \"s={}, accuracy={:.3f}\".format(xmax, ymax)\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "    arrowprops = dict(arrowstyle=\"->\",connectionstyle=\"angle,angleA=0,angleB=-60\")\n",
    "    kw = dict(xycoords='data',textcoords=\"axes fraction\",\n",
    "              arrowprops=arrowprops, bbox=bbox_props, ha=\"right\", va=\"top\")\n",
    "    ax.annotate(text, xy=(xmax, ymax), xytext=(0.85,0.50), **kw)\n",
    "\n",
    "annot_max(layer_size_range, mean_scores)\n",
    "\n",
    "plt.xlabel('Value of s for hidden layer size')\n",
    "plt.ylabel('Cross-validated accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means that if we choose arbitrary values for hyperparameters, it is likely that performance will suffer. Still, we would not want to search for optimal values manually, `scikit-learn` offers methods to automatically search for them.\n",
    "\n",
    "**Question for you: in your opinion, is the choice of classifier itself a hyperparameter that should be optimized?**\n",
    "\n",
    "### Grid search for hyperparameter values\n",
    "\n",
    "A straightforward way to search for an optimal hyperparameter value (\"hyperparameter tuning\") is to define the set of values that should be tried and from this set choose the one that gives the highest cross-validation score. If several hyperparameters need to be optimized, all permutations need to be tried. This method is called **grid search**.\n",
    "\n",
    "We now employ grid search with cross-validation to automate the procedure above that finds an optimal value for MLP hidden layer size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': [20, 30, 40, 50, 60, 70, 80, 90, 100]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=2,\n",
       "       param_grid={'hidden_layer_sizes': [20, 30, 40, 50, 60, 70, 80, 90, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "layer_size_range = range(20, 110, 10)\n",
    "\n",
    "# define parameters whose value space needs to be searched\n",
    "param_grid = {'hidden_layer_sizes': layer_size_range}\n",
    "print param_grid\n",
    "\n",
    "# create estimator object, default values\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "# instantiate grid search class\n",
    "grid = GridSearchCV(mlp, param_grid, cv=10, scoring='accuracy', n_jobs=2, return_train_score=True)\n",
    "\n",
    "# fit iris data (reload if you modified those variables)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are stored in `grid.cv_results_`, which can be converted to a pandas data frame for inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.144219</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.978519</td>\n",
       "      <td>100</td>\n",
       "      <td>{u'hidden_layer_sizes': 100}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.030551</td>\n",
       "      <td>0.005185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.096226</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.971852</td>\n",
       "      <td>80</td>\n",
       "      <td>{u'hidden_layer_sizes': 80}</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.006933</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.044222</td>\n",
       "      <td>0.005543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.087924</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.971852</td>\n",
       "      <td>60</td>\n",
       "      <td>{u'hidden_layer_sizes': 60}</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.044721</td>\n",
       "      <td>0.005543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.090802</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>70</td>\n",
       "      <td>{u'hidden_layer_sizes': 70}</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.053748</td>\n",
       "      <td>0.008114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.147408</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975556</td>\n",
       "      <td>90</td>\n",
       "      <td>{u'hidden_layer_sizes': 90}</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.006476</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.044721</td>\n",
       "      <td>0.006667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.086528</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.965185</td>\n",
       "      <td>50</td>\n",
       "      <td>{u'hidden_layer_sizes': 50}</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.061101</td>\n",
       "      <td>0.009399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.096704</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.964444</td>\n",
       "      <td>40</td>\n",
       "      <td>{u'hidden_layer_sizes': 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.014279</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.058119</td>\n",
       "      <td>0.013168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.090175</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.910370</td>\n",
       "      <td>30</td>\n",
       "      <td>{u'hidden_layer_sizes': 30}</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.874074</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.770370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.075719</td>\n",
       "      <td>0.064444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.077719</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.823704</td>\n",
       "      <td>20</td>\n",
       "      <td>{u'hidden_layer_sizes': 20}</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.725926</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.725926</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.896296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.003095</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.127366</td>\n",
       "      <td>0.096399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "8       0.144219         0.000438         0.980000          0.978519   \n",
       "6       0.096226         0.000298         0.973333          0.971852   \n",
       "4       0.087924         0.000294         0.966667          0.971852   \n",
       "5       0.090802         0.000281         0.966667          0.970370   \n",
       "7       0.147408         0.000422         0.966667          0.975556   \n",
       "3       0.086528         0.000284         0.960000          0.965185   \n",
       "2       0.096704         0.000331         0.946667          0.964444   \n",
       "1       0.090175         0.000302         0.926667          0.910370   \n",
       "0       0.077719         0.000364         0.833333          0.823704   \n",
       "\n",
       "  param_hidden_layer_sizes                        params  rank_test_score  \\\n",
       "8                      100  {u'hidden_layer_sizes': 100}                1   \n",
       "6                       80   {u'hidden_layer_sizes': 80}                2   \n",
       "4                       60   {u'hidden_layer_sizes': 60}                3   \n",
       "5                       70   {u'hidden_layer_sizes': 70}                3   \n",
       "7                       90   {u'hidden_layer_sizes': 90}                3   \n",
       "3                       50   {u'hidden_layer_sizes': 50}                6   \n",
       "2                       40   {u'hidden_layer_sizes': 40}                7   \n",
       "1                       30   {u'hidden_layer_sizes': 30}                8   \n",
       "0                       20   {u'hidden_layer_sizes': 20}                9   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "8                1.0            0.977778           1.000000   \n",
       "6                1.0            0.977778           1.000000   \n",
       "4                1.0            0.970370           1.000000   \n",
       "5                1.0            0.948148           1.000000   \n",
       "7                1.0            0.970370           0.933333   \n",
       "3                1.0            0.962963           1.000000   \n",
       "2                1.0            0.955556           1.000000   \n",
       "1                1.0            0.955556           1.000000   \n",
       "0                1.0            0.940741           1.000000   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  \\\n",
       "8            0.985185           1.000000            0.977778   \n",
       "6            0.970370           1.000000            0.962963   \n",
       "4            0.977778           1.000000            0.970370   \n",
       "5            0.970370           1.000000            0.970370   \n",
       "7            0.962963           1.000000            0.970370   \n",
       "3            0.962963           1.000000            0.955556   \n",
       "2            0.955556           1.000000            0.970370   \n",
       "1            0.970370           0.800000            0.859259   \n",
       "0            0.948148           0.733333            0.725926   \n",
       "\n",
       "   split3_test_score  split3_train_score  split4_test_score  \\\n",
       "8           0.933333            0.977778           0.933333   \n",
       "6           0.933333            0.970370           0.933333   \n",
       "4           0.933333            0.977778           0.866667   \n",
       "5           0.933333            0.970370           0.866667   \n",
       "7           0.933333            0.970370           0.866667   \n",
       "3           0.866667            0.948148           0.866667   \n",
       "2           0.933333            0.970370           0.866667   \n",
       "1           0.933333            0.977778           0.800000   \n",
       "0           0.733333            0.725926           0.666667   \n",
       "\n",
       "   split4_train_score  split5_test_score  split5_train_score  \\\n",
       "8            0.985185           1.000000            0.977778   \n",
       "6            0.977778           1.000000            0.977778   \n",
       "4            0.962963           0.933333            0.970370   \n",
       "5            0.977778           1.000000            0.977778   \n",
       "7            0.985185           1.000000            0.977778   \n",
       "3            0.977778           1.000000            0.970370   \n",
       "2            0.985185           0.866667            0.948148   \n",
       "1            0.851852           0.866667            0.874074   \n",
       "0            0.688889           0.800000            0.859259   \n",
       "\n",
       "   split6_test_score  split6_train_score  split7_test_score  \\\n",
       "8           0.933333            0.985185           1.000000   \n",
       "6           0.866667            0.962963           1.000000   \n",
       "4           0.933333            0.977778           1.000000   \n",
       "5           0.866667            0.977778           1.000000   \n",
       "7           0.933333            0.985185           1.000000   \n",
       "3           0.866667            0.977778           1.000000   \n",
       "2           0.866667            0.985185           0.933333   \n",
       "1           0.933333            0.770370           1.000000   \n",
       "0           0.666667            0.703704           0.866667   \n",
       "\n",
       "   split7_train_score  split8_test_score  split8_train_score  \\\n",
       "8            0.977778           1.000000            0.970370   \n",
       "6            0.970370           1.000000            0.970370   \n",
       "4            0.962963           1.000000            0.970370   \n",
       "5            0.970370           1.000000            0.970370   \n",
       "7            0.977778           1.000000            0.977778   \n",
       "3            0.970370           1.000000            0.955556   \n",
       "2            0.955556           1.000000            0.948148   \n",
       "1            0.955556           0.933333            0.940741   \n",
       "0            0.859259           0.866667            0.896296   \n",
       "\n",
       "   split9_test_score  split9_train_score  std_fit_time  std_score_time  \\\n",
       "8                1.0            0.970370      0.008386        0.000047   \n",
       "6                1.0            0.977778      0.006933        0.000044   \n",
       "4                1.0            0.977778      0.004070        0.000033   \n",
       "5                1.0            0.970370      0.002809        0.000004   \n",
       "7                1.0            0.977778      0.006476        0.000030   \n",
       "3                1.0            0.970370      0.003677        0.000021   \n",
       "2                1.0            0.970370      0.014279        0.000082   \n",
       "1                1.0            0.948148      0.012600        0.000029   \n",
       "0                1.0            0.888889      0.003095        0.000112   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "8        0.030551         0.005185  \n",
       "6        0.044222         0.005543  \n",
       "4        0.044721         0.005543  \n",
       "5        0.053748         0.008114  \n",
       "7        0.044721         0.006667  \n",
       "3        0.061101         0.009399  \n",
       "2        0.058119         0.013168  \n",
       "1        0.075719         0.064444  \n",
       "0        0.127366         0.096399  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a bit unwieldy\n",
    "# print grid.cv_results_\n",
    "\n",
    "# better with pandas\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(grid.cv_results_)\n",
    "df.sort_values(by=[\"rank_test_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the course of grid search, the grid is automatically refit with the best parameter values and all of the training examples (so that you would not waste data). Predictions can be made directly with the `grid` objec, in the usual fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.predict([[0.2, 0.5, 0.6, 0.8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search as an efficient approximation to grid search\n",
    "\n",
    "For large models, a high number of parameters or high range of values, grid search can be computationally expensive. Searching for values would be more efficient if it were not _exhaustive_, that is, if only a subset of value combinations would be cross-validated. Such a method is called **random(ized) search**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=2,\n",
       "          param_distributions={'solver': ['lbfgs', 'sgd', 'adam'], 'learning_rate_init': [0.1, 0.01, 0.001, 0.0001], 'hidden_layer_sizes': [20, 30, 40, 50, 60, 70, 80, 90, 100]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# bigger search space\n",
    "layer_size_range = range(20, 110, 10)\n",
    "solver_values = ['lbfgs', 'sgd', 'adam']\n",
    "learning_rate_init_values = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "# define parameters whose value space needs to be searched\n",
    "param_grid = {'hidden_layer_sizes': layer_size_range,\n",
    "              'solver': solver_values,\n",
    "              'learning_rate_init': learning_rate_init_values}\n",
    "\n",
    "random_search = RandomizedSearchCV( estimator=mlp,\n",
    "                                    param_distributions=param_grid,\n",
    "                                    n_iter=10,\n",
    "                                    cv=10,\n",
    "                                    scoring='accuracy',\n",
    "                                    n_jobs=2,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=42)\n",
    "\n",
    "# fit iris data (reload if you modified those variables)\n",
    "random_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>param_learning_rate_init</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.051162</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.987407</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{u'hidden_layer_sizes': 50, u'learning_rate_init': 0.0001, u'solver': u'lbfgs'}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.992593</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.021817</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.032660</td>\n",
       "      <td>0.004743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.047680</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.988148</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{u'hidden_layer_sizes': 40, u'learning_rate_init': 0.001, u'solver': u'lbfgs'}</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.992593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992593</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.992593</td>\n",
       "      <td>0.013288</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.044222</td>\n",
       "      <td>0.005926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.080411</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.945185</td>\n",
       "      <td>70</td>\n",
       "      <td>0.001</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{u'hidden_layer_sizes': 70, u'learning_rate_init': 0.001, u'solver': u'sgd'}</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.903704</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.004196</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.019373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.060168</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>adam</td>\n",
       "      <td>{u'hidden_layer_sizes': 80, u'learning_rate_init': 0.01, u'solver': u'adam'}</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.992593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.019232</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.102415</td>\n",
       "      <td>0.095613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.056428</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.918519</td>\n",
       "      <td>20</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{u'hidden_layer_sizes': 20, u'learning_rate_init': 0.01, u'solver': u'sgd'}</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.992593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.026833</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.130128</td>\n",
       "      <td>0.126013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016109</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>70</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{u'hidden_layer_sizes': 70, u'learning_rate_init': 0.1, u'solver': u'adam'}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.585185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.019641</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.206989</td>\n",
       "      <td>0.219164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.090645</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.682222</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{u'hidden_layer_sizes': 80, u'learning_rate_init': 0.0001, u'solver': u'adam'}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.637037</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.659259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.674074</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.725926</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.651852</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.134164</td>\n",
       "      <td>0.103091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066449</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.593333</td>\n",
       "      <td>0.617037</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{u'hidden_layer_sizes': 20, u'learning_rate_init': 0.0001, u'solver': u'sgd'}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.659259</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.674074</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.437037</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.153333</td>\n",
       "      <td>0.141015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.143064</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.561481</td>\n",
       "      <td>90</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{u'hidden_layer_sizes': 90, u'learning_rate_init': 0.0001, u'solver': u'adam'}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.429630</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.681481</td>\n",
       "      <td>0.007206</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.137275</td>\n",
       "      <td>0.147063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.085910</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.329630</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{u'hidden_layer_sizes': 20, u'learning_rate_init': 0.0001, u'solver': u'adam'}</td>\n",
       "      <td>10</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.562963</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.140741</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.018559</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.089443</td>\n",
       "      <td>0.097556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "7       0.051162         0.000311         0.973333          0.987407   \n",
       "6       0.047680         0.000315         0.960000          0.988148   \n",
       "5       0.080411         0.000282         0.953333          0.945185   \n",
       "0       0.060168         0.000373         0.946667          0.953333   \n",
       "2       0.056428         0.000280         0.913333          0.918519   \n",
       "4       0.016109         0.000303         0.706667          0.688889   \n",
       "3       0.090645         0.000292         0.700000          0.682222   \n",
       "1       0.066449         0.000300         0.593333          0.617037   \n",
       "8       0.143064         0.000408         0.560000          0.561481   \n",
       "9       0.085910         0.000326         0.333333          0.329630   \n",
       "\n",
       "  param_hidden_layer_sizes param_learning_rate_init param_solver  \\\n",
       "7                       50                   0.0001        lbfgs   \n",
       "6                       40                    0.001        lbfgs   \n",
       "5                       70                    0.001          sgd   \n",
       "0                       80                     0.01         adam   \n",
       "2                       20                     0.01          sgd   \n",
       "4                       70                      0.1         adam   \n",
       "3                       80                   0.0001         adam   \n",
       "1                       20                   0.0001          sgd   \n",
       "8                       90                   0.0001         adam   \n",
       "9                       20                   0.0001         adam   \n",
       "\n",
       "                                                                            params  \\\n",
       "7  {u'hidden_layer_sizes': 50, u'learning_rate_init': 0.0001, u'solver': u'lbfgs'}   \n",
       "6   {u'hidden_layer_sizes': 40, u'learning_rate_init': 0.001, u'solver': u'lbfgs'}   \n",
       "5     {u'hidden_layer_sizes': 70, u'learning_rate_init': 0.001, u'solver': u'sgd'}   \n",
       "0     {u'hidden_layer_sizes': 80, u'learning_rate_init': 0.01, u'solver': u'adam'}   \n",
       "2      {u'hidden_layer_sizes': 20, u'learning_rate_init': 0.01, u'solver': u'sgd'}   \n",
       "4      {u'hidden_layer_sizes': 70, u'learning_rate_init': 0.1, u'solver': u'adam'}   \n",
       "3   {u'hidden_layer_sizes': 80, u'learning_rate_init': 0.0001, u'solver': u'adam'}   \n",
       "1    {u'hidden_layer_sizes': 20, u'learning_rate_init': 0.0001, u'solver': u'sgd'}   \n",
       "8   {u'hidden_layer_sizes': 90, u'learning_rate_init': 0.0001, u'solver': u'adam'}   \n",
       "9   {u'hidden_layer_sizes': 20, u'learning_rate_init': 0.0001, u'solver': u'adam'}   \n",
       "\n",
       "   rank_test_score  split0_test_score  split0_train_score  split1_test_score  \\\n",
       "7                1           1.000000            0.985185           1.000000   \n",
       "6                2           1.000000            0.985185           0.933333   \n",
       "5                3           1.000000            0.955556           0.866667   \n",
       "0                4           1.000000            0.985185           1.000000   \n",
       "2                5           1.000000            0.977778           0.666667   \n",
       "4                6           0.666667            0.666667           0.666667   \n",
       "3                7           0.666667            0.637037           0.666667   \n",
       "1                8           0.533333            0.659259           0.666667   \n",
       "8                9           0.666667            0.666667           0.666667   \n",
       "9               10           0.333333            0.333333           0.333333   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  \\\n",
       "7            0.985185           1.000000            0.985185   \n",
       "6            0.985185           1.000000            0.985185   \n",
       "5            0.903704           1.000000            0.948148   \n",
       "0            0.985185           1.000000            0.985185   \n",
       "2            0.666667           1.000000            0.977778   \n",
       "4            0.585185           1.000000            0.977778   \n",
       "3            0.659259           1.000000            0.925926   \n",
       "1            0.755556           0.800000            0.696296   \n",
       "8            0.666667           0.466667            0.488889   \n",
       "9            0.259259           0.333333            0.333333   \n",
       "\n",
       "   split3_test_score  split3_train_score  split4_test_score  \\\n",
       "7           0.933333            0.992593           0.933333   \n",
       "6           0.933333            0.992593           1.000000   \n",
       "5           0.933333            0.962963           0.866667   \n",
       "0           1.000000            0.985185           0.933333   \n",
       "2           1.000000            0.977778           0.866667   \n",
       "4           0.666667            0.629630           0.333333   \n",
       "3           0.666667            0.674074           0.466667   \n",
       "1           0.733333            0.674074           0.533333   \n",
       "8           0.666667            0.703704           0.333333   \n",
       "9           0.333333            0.333333           0.333333   \n",
       "\n",
       "   split4_train_score  split5_test_score  split5_train_score  \\\n",
       "7            0.985185           0.933333            0.985185   \n",
       "6            0.992593           0.933333            0.977778   \n",
       "5            0.940741           1.000000            0.955556   \n",
       "0            0.985185           1.000000            0.977778   \n",
       "2            0.992593           1.000000            0.985185   \n",
       "4            0.333333           0.933333            0.985185   \n",
       "3            0.481481           0.866667            0.725926   \n",
       "1            0.577778           0.733333            0.822222   \n",
       "8            0.333333           0.466667            0.429630   \n",
       "9            0.333333           0.333333            0.333333   \n",
       "\n",
       "   split6_test_score  split6_train_score  split7_test_score  \\\n",
       "7           0.933333            1.000000           1.000000   \n",
       "6           0.866667            1.000000           1.000000   \n",
       "5           0.866667            0.977778           1.000000   \n",
       "0           0.866667            0.992593           1.000000   \n",
       "2           0.933333            0.985185           0.666667   \n",
       "4           0.466667            0.400000           0.666667   \n",
       "3           0.666667            0.651852           0.666667   \n",
       "1           0.733333            0.711111           0.400000   \n",
       "8           0.333333            0.311111           0.666667   \n",
       "9           0.533333            0.562963           0.333333   \n",
       "\n",
       "   split7_train_score  split8_test_score  split8_train_score  \\\n",
       "7            0.985185           1.000000            0.985185   \n",
       "6            0.985185           1.000000            0.985185   \n",
       "5            0.940741           1.000000            0.940741   \n",
       "0            0.985185           0.666667            0.666667   \n",
       "2            0.666667           1.000000            0.977778   \n",
       "4            0.666667           0.666667            0.666667   \n",
       "3            0.688889           0.666667            0.711111   \n",
       "1            0.437037           0.333333            0.370370   \n",
       "8            0.666667           0.666667            0.666667   \n",
       "9            0.333333           0.133333            0.140741   \n",
       "\n",
       "   split9_test_score  split9_train_score  std_fit_time  std_score_time  \\\n",
       "7           1.000000            0.985185      0.021817        0.000043   \n",
       "6           0.933333            0.992593      0.013288        0.000044   \n",
       "5           1.000000            0.925926      0.004196        0.000012   \n",
       "0           1.000000            0.985185      0.019232        0.000156   \n",
       "2           1.000000            0.977778      0.026833        0.000043   \n",
       "4           1.000000            0.977778      0.019641        0.000057   \n",
       "3           0.666667            0.666667      0.003892        0.000044   \n",
       "1           0.466667            0.466667      0.003120        0.000080   \n",
       "8           0.666667            0.681481      0.007206        0.000013   \n",
       "9           0.333333            0.333333      0.018559        0.000086   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "7        0.032660         0.004743  \n",
       "6        0.044222         0.005926  \n",
       "5        0.060000         0.019373  \n",
       "0        0.102415         0.095613  \n",
       "2        0.130128         0.126013  \n",
       "4        0.206989         0.219164  \n",
       "3        0.134164         0.103091  \n",
       "1        0.153333         0.141015  \n",
       "8        0.137275         0.147063  \n",
       "9        0.089443         0.097556  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the results\n",
    "pd.options.display.max_colwidth = 100 \n",
    "\n",
    "df = pd.DataFrame.from_dict(random_search.cv_results_)\n",
    "df.sort_values(by=[\"rank_test_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a worthwhile article about the differences between grid search and random search: https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881.\n",
    "\n",
    "### Quick access to best estimator and parameters\n",
    "\n",
    "Instead of a tabular overview, you can of course quickly access the best estimator and the mean score (mean over all cross-validated folds) obtained by this estimator. Here are some useful outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=50, learning_rate='constant',\n",
      "       learning_rate_init=0.0001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "0.973333333333\n",
      "{'hidden_layer_sizes': 50, 'learning_rate_init': 0.0001, 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "print random_search.best_estimator_ # convention: variables computed by `scikit-learn` end in \"_\"\n",
    "print random_search.best_score_\n",
    "print random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Splitting data into training, development and testing\n",
    "\n",
    "Until now, we have split data into training and testing. We used test data for parameter optimization. By the time we arrive at optimal parameter values, we will have had many passes through test data. Because of that, some people would argue, the final accuracy score is not really _out-of-sample_.\n",
    "\n",
    "An even better estimate of true generalization error is obtained when the test set is truly held out, i.e. never shown to the model until the final testing round. In `scikit-learn`, this means splitting the data into training and testing parts, and then do search and cross-validation on the training part only. During cross-validation, a fraction of the training set will implicitly become the _development_ or _validation_ set.\n",
    "\n",
    "**Task for you: implement the idea described in the paragraph above, using the `iris` data set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Outlook\n",
    "\n",
    "This concludes the story of machine learning metrics, model selection and evaluation. There are many ways to build on your current knowledge, here are some ideas:\n",
    "\n",
    "- **feature extraction:** evaluation and optimization are important, but so are the _features_ that represent samples / observations. Each observation must essentially be a list of numbers (a numpy array). If your features are not numbers, how do you transform them into numbers?\n",
    "- **normalization:** there are differences in the data that do not matter for your machine learning task. Normalization means smoothing away unwanted variance in the data.\n",
    "- **scaling**: some algorithms are sensitive to feature scaling. A straightforward transformation of all the values can help to obtain a distribution with \"zero mean and unit variance\" (if it were a normal distribution: N(0, 1)).\n",
    "- **data augmentation:** if training data for your task is scarce, think about artificially augmenting your small set of labelled observations.\n",
    "- **model ensembling:** training models introduces a certain amount of _variance_. An effective way of fighting that variance is training several models and make predictions with all of those models, then take the majority vote for instance. Here is a good article about bias and variance in statistical models: http://scott.fortmann-roe.com/docs/BiasVariance.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
