{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to working with `scikit-learn`, _the_ machine learning library for Python.\n",
    "\n",
    "## 0 Setting up your system\n",
    "\n",
    "Here is the recommended way to set up your system:\n",
    "\n",
    "- install `miniconda`, get it from: https://conda.io/miniconda.html\n",
    "- use it to install at least: `numpy`, `scikit-learn`, `seaborn`, `matplotlib`, `pandas`:\n",
    "\n",
    "```\n",
    "conda install numpy, scikit-learn, seaborn, matplotlib, pandas\n",
    "```\n",
    "\n",
    "- install `jupyter`, if you do not have it yet - also with `conda`:\n",
    "\n",
    "```\n",
    "conda install jupyter\n",
    "```\n",
    "\n",
    "- start up jupyter with:\n",
    "```  \n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "- this should open your browser automatically, if not open a browser window and navigate to localhost with the specified port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Loading Datasets\n",
    "\n",
    "This sections explains how you can load datasets into `scikit-learn`.\n",
    "\n",
    "### Loading a toy data set\n",
    "\n",
    "`scikit-learn` ships with a number of toy datasets that you can simply import. Here is an example, the IRIS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data loading function (without actually loading the data yet)\n",
    "from sklearn.datasets import load_iris\n",
    "load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and assign it to a variable\n",
    "iris = load_iris() # convention: variable name describes data set, instead of generic\n",
    "type(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All toy datasets come in the same format and have the same attributes (well, at least `data` and `target`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what attributes does the `data` variable have?\n",
    "dir(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# familiarize yourself with the data set before you do anything else\n",
    "# note: see how statistics is relevant here?\n",
    "print iris.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data` attribute is a list of lists, where the outer list is a list of observations (\"samples\"). Each observation is itself a list - a list of features. Or, more precisely, an observation is a list of feature **values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the feature values of the first observation\n",
    "iris.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do those feature values correspond to? The `feature_names` attribute explains this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important convention: list of observations is assigned to the variable `X`, the list of targets (or labels, or responses, or classes, even though that last one is sloppy) is assigned to `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target\n",
    "# first 10 observations together with their class:\n",
    "print \"{}\\t\\t{}\".format(\"Observation\", \"Response\")\n",
    "for observation, response in zip(X[:10], y[:10]):\n",
    "    print \"{}\\t{}\".format(observation, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a response of 0 mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print set(y)\n",
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question for you: why is `X` uppercase, when `y` is not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the observations and responses are of type `numpy.ndarray`, a flexible container for scalars, vectors and matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ndarray`s have a convenient attribute `shape` that gives you the dimensions of the object in question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you: what do those numbers mean? Which of them mean columns, which mean rows?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an external dataset in a standard format like CSV\n",
    "\n",
    "Loading an existing dataset for use with `scikit-learn` is also easy. We use a handy library called `pandas` that lets you manipulate data in an R-like fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# always import as `pd`, this is a convention\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import an example dataset from ICS UCI, find more information here: https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read CSV directly from URL\n",
    "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables created in this way are `pandas` `DataFrame` objects that have a variety of convenient attributes, for instance the `head` or `tail` or `describe` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print type(data)\n",
    "# to display all other attributes of the data frame: dir(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, something is not quite right. The first observation is mistaken as the row of column headers. Fixing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define column names\n",
    "col_names = ['patient_age', 'operation_year', 'auxiliary_node_count', 'survival_status']\n",
    "\n",
    "# read CSV directly from URL, indicate that there are no headers\n",
    "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\", header=None, names=col_names)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data frames also have a shape attribute and named columns can be accessed by their name (\"bracket notation\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print data.shape\n",
    "# access last column directly\n",
    "print data.survival_status.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you: Again, what does the output of `shape` mean, in your opinion?**\n",
    "\n",
    "As with a toy dataset, the convention is to assign the data to `X` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define columns that are features\n",
    "feature_cols = ['patient_age', 'operation_year', 'auxiliary_node_count']\n",
    "X = data[feature_cols]\n",
    "y = data.survival_status # alternatively, data['survival_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X.head()\n",
    "print \n",
    "print y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data that is neither toy nor in a standardized format\n",
    "\n",
    "If your data is none of the above, it is your job to process it until you can assign it to `X` and `y`, both of which must be of type `numpy.ndarray`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Visualizing datasets\n",
    "\n",
    "Once data is loaded into memory, it can be useful to visualize it, before you train any machine learning system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns # convention, always import as `sns`\n",
    "import matplotlib.pyplot as plt # same here\n",
    "\n",
    "# matplotlib \"magic\" command\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing regression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a new dataset, this time a REGRESSION problem:\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "# convert to data frame, check output\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['price'] = boston.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to get insight from regression data is a so-called \"pair plot\" that emphasizes the _pairwise_ relationships in your dataset. Seaborn can generate such plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression plot for only the first 3 features, plotted against the target variable\n",
    "sns.pairplot(df, x_vars=boston.feature_names[:3], y_vars='price', size=7, aspect=0.7, kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out http://www.neural.cz/dataset-exploration-boston-house-pricing.html for more cool examples specifically with the Boston House Pricing data set and the libraries we have used.\n",
    "\n",
    "Another example, taken from the seaborn pairplot documentation (http://seaborn.pydata.org/generated/seaborn.pairplot.html):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing classification data\n",
    "\n",
    "If your data does not describe a regression problem (i.e. if the response variable is not continuous, but categorical), then regression plots do not make much sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris() # function from scikit-learn\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "# `df` is the conventional name for a dataframe variable\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variable is only used for coloring, both x axis and y axis are feature names\n",
    "sns.pairplot(df, hue=\"species\", size=3, x_vars=iris.feature_names, y_vars=iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot to discover in those plots, for instance:\n",
    "- how well individual features can tell apart observations of different classes\n",
    "- in the diagonal: how feature values are distributed overall, and in each class\n",
    "\n",
    "If you're interested, find out more: http://seaborn.pydata.org/tutorial.html, http://pandas.pydata.org/ and http://scikit-learn.org/stable/datasets/index.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training and evaluating a classifier\n",
    "\n",
    "This sections explains how simple training and evaluation work in `scikit-learn`.\n",
    "\n",
    "### Your first classifier\n",
    "\n",
    "Once you have `X` and `y` and have some intuitions about your data, train your first classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review, make sure correct dataset is loaded\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` offers a range of different classifiers, they are organized into different modules. A classifier we already know is KNN (\"k nearest neighbor\"). \n",
    "\n",
    "Classifier classes are imported from modules, then instantiated. Simply printing the object gives you all parameters of the classifiers, including implicit defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=5) # convention: call classifier instance `clf`, or describe estimator type\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, call the `fit` method to actually learn the relationship between `X` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you: In the case of KNN, what does \"learning\" mean?**\n",
    "\n",
    "After a model is \"fitted\", you can predict the reponse for new observations. The input must be a list of lists, exactly the same as `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict([[0.2, 0.4, 0.5, 0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you: In the case of KNN, what does \"predicting\" mean? And: What does this output mean?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple training and testing split\n",
    "\n",
    "Since the ultimate goal is to _generalize_ well (this is **extremely** important), there must be a way to evaluate if our model performs well on unseen data. The first method we look at is to simply hold out part of the data (meaning: not show it to the classifier during training), and then evaluate the model on the held-out data.\n",
    "\n",
    "Split up your observations and responses into a training and testing part each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# variable names are conventional, please also adopt them\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # 20 percent go into the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in (X_train, X_test, y_train, y_test):\n",
    "    print v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to evaluate properly, we need to train a classifier that has only seen the training part of the data, and is oblivious to the correct test set answers.\n",
    "\n",
    "Then, predict the correct answers for the test observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test) # always use `y_pred` as the variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you: What is the difference between `y_test` and `y_pred`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare `y_test` and `y_pred`:\n",
    "print \"Actual\\tPredicted\"\n",
    "for t, p in zip(y_test[:10], y_pred[:10]):\n",
    "    print \"{}\\t{}\".format(t, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation by calculating accuracy\n",
    "\n",
    "Now that we have the predictions for the test set examples, and the true answers, we can evaluate automatically the performance of the classifier. There are several different reasonable metrics, accuracy is by far the simplest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics # module dedicated to measuring performance\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you: How is accuracy computed?**\n",
    "\n",
    "If you cannot answer this question, look it up. Then try to implement an accuracy function yourself: one that takes `y_pred` and `y_test` as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Outlook\n",
    "\n",
    "This method of model fitting and evaluation has several problems, which we will discuss in later classes. To give you some food for thought:\n",
    "- We have split the data randomly into training and testing examples. Therefore, it is possible that the test only contains \"easy\" examples or only hard ones. Isn't this a bit unfair?\n",
    "- Each classifier has hyperparameters that need to be set by the user. For instance, `n_neighbors` in our case. We have set it to `5`, an arbitrary decision. Can we do better than that?\n",
    "- Does accuracy work for a regression problem? Why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
